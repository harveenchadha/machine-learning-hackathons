{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_file(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "    print(df.info())\n",
    "    return  df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_csv_file('train.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fil_cols = [ 'account_info' , 'credit_history' , 'purpose' , 'savings_account', 'employment_st', 'personal_status', 'gurantors', 'property_type' ,'installment_type' ,'housing_type' ,'job_type', 'telephone' , 'foreigner' , 'serial number' , 'duration_month',  'poi' , 'resident_since' , 'age', 'credits_no', 'liables', 'credit_amount']\n",
    "len(fil_cols)\n",
    "new_fil_cols =[ 'account_info' , 'credit_history' , 'purpose' , 'savings_account', 'employment_st',  'gurantors', 'property_type' ,'installment_type' ,'housing_type' ,'job_type' , 'foreigner' , 'serial number' , 'duration_month',  'poi' , 'resident_since' , 'age', 'credits_no', 'liables', 'credit_amount']\n",
    "new_val_cols =[ 'account_info' , 'credit_history' , 'purpose' , 'savings_account', 'employment_st',  'gurantors', 'property_type' ,'installment_type' ,'housing_type' ,'job_type' , 'foreigner' , 'serial number' , 'duration_month',  'poi' , 'resident_since' , 'age', 'credits_no', 'liables']\n",
    "\n",
    "val_cols = [ 'account_info' , 'credit_history' , 'purpose' , 'savings_account', 'employment_st', 'personal_status', 'gurantors', 'property_type' ,'installment_type' ,'housing_type' ,'job_type', 'telephone' , 'foreigner' , 'serial number' , 'duration_month',  'poi' , 'resident_since' , 'age', 'credits_no', 'liables']\n",
    "obj_cols = [ 'account_info' , 'credit_history' , 'purpose' , 'savings_account', 'employment_st', 'personal_status', 'gurantors', 'property_type' ,'installment_type' ,'housing_type' ,'job_type', 'telephone' , 'foreigner' ]\n",
    "\n",
    "new_obj_cols = [ 'account_info' , 'credit_history' , 'purpose' , 'savings_account', 'employment_st',  'gurantors', 'property_type' ,'installment_type' ,'housing_type' ,'job_type' , 'foreigner' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorder_columns(fil_cols, df):\n",
    "    df =df[fil_cols]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =reorder_columns(fil_cols, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['credit_amount'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_columns(df , columns):\n",
    "    new_df = df.drop(columns =columns, axis = 1)\n",
    "    return new_df\n",
    "columns_to_be_dropped = ['serial number','telephone','personal_status']\n",
    "new_df = drop_columns(df, columns_to_be_dropped)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_df = pd.get_dummies(new_df, columns = new_obj_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_df = new_df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFeatures(new_df):\n",
    "    labels = new_df['credit_amount']\n",
    "    X = drop_columns(new_df,'credit_amount')\n",
    "    return np.array(X), np.array(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X , y = getFeatures(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0] ,y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_valid , y_train, y_valid = train_test_split(X, y, test_size =0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape , y_train.shape , X_valid.shape , y_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import PCA\n",
    "# pca= PCA()\n",
    "# pca.fit(X_train)\n",
    "# X_train = pca.transform(X_train)\n",
    "# X_valid = pca.transform(X_valid)\n",
    "\n",
    "# X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scale = MinMaxScaler()\n",
    "scale.fit(X_train)\n",
    "X_train = scale.transform(X_train)\n",
    "X_valid = scale.transform(X_valid)\n",
    "X_train.shape, X_valid.shape\n",
    "# y_valid_scaled = scale_y.transform(y_valid_new)\n",
    "# #y_valid = minmax(y_valid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train_new = y_train.reshape(-1,1)\n",
    "# y_valid_new = y_valid.reshape(-1,1)\n",
    "# y_train_new.shape,y_valid_new.shape\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# scale_y = MinMaxScaler()\n",
    "# scale_y.fit(y_train_new)\n",
    "# y_train_scaled = scale_y.transform(y_train_new)\n",
    "# y_valid_scaled = scale_y.transform(y_valid_new)\n",
    "# #y_valid = minmax(y_valid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train_scaled[3] , y_valid_scaled[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers.core import Flatten,Dense,Dropout, Activation, Lambda\n",
    "from keras.models import Model, Sequential\n",
    "from keras.optimizers import Adam,SGD\n",
    "from keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return (K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "#model.add(Lambda(lambda x: x , input_shape = (24)))\n",
    "#model.add(Lambda(lambda x: x+0.1 , input_shape = (64,64,3)))\n",
    "#model.add(Flatten())\n",
    "model.add(BatchNormalization(input_shape=(54,)))\n",
    "model.add(Dense(100))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(200))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(100))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('linear'))\n",
    "model.compile(optimizer = 'Adam' , loss = root_mean_squared_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, epochs=500, verbose =1, validation_data=(X_valid, y_valid), batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./reg_model_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('./test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = reorder_columns(val_cols,test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test_df = pd.get_dummies(test_df, columns = obj_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test_df = drop_columns(new_test_df, columns_to_be_dropped)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.array(new_test_df)\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_scaled = pca.transform(X_test)\n",
    "X_test_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred = scale_y.inverse_transform(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_csv(pred_test, filename):\n",
    "    df = pd.read_csv('sample_reg.csv')\n",
    "    df['credit_amount'] = pred_test\n",
    "    df.to_csv(filename,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_csv(y_pred, 'regression_v3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classification_labels(row):\n",
    "#for row in new_df.itertuples():\n",
    "    #print(row.credit_amount)\n",
    "    if(row < 1500):\n",
    "        return 3\n",
    "    elif(row >= 1500 and row < 4000):\n",
    "        return 2\n",
    "    elif (row >= 4000 and row < 20000):\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_csv_new( filename):\n",
    "    df = pd.read_csv('regression_v3.csv')\n",
    "    df['cluster_number'] = df['credit_amount'].apply(get_classification_labels)\n",
    "    df = df.drop(columns=['credit_amount'])\n",
    "    df.to_csv(filename,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_csv_new('classification_v2.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
